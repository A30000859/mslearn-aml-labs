{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting and Mitigating Unfairness in Models\n",
    "\n",
    "Machine learning models can incorporate unintentional bias, which can lead to issues with *fairness*. For example, a model that predicts the likelihood of diabetes might work well for some age groups, but not for others - subjecting a subset of patients to unnecessary tests, or depriving them of tests that would confirm a diabetes diagnosis.\n",
    "\n",
    "In this exercise, you'll use the **FairLearn** package to analyze a model and find any disparity in prediction performance for different subsets of patients based on age.\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "Before you start this lab, ensure that you have completed the *Create an Azure Machine Learning Workspace* and *Create a Compute Instance* tasks in [Lab 1: Getting Started with Azure Machine Learning](./labdocs/Lab01.md). Then open Jupyter on your Compute Instance and create a new **Terminal**.\n",
    "\n",
    "The FairLearn package used in this exercise has dependencies on specific versions of common Python packages. To avoid potential conflicts, you're going to create a separate Conda environment and associated Jupyter kernel specifically for this exercise.\n",
    "\n",
    "In the terminal, run the following commands to create a new Conda environment and Jupyter kernel that incldues all of the packages you need to work with FairLearn.\n",
    "\n",
    "```\n",
    "conda create -y -n fair python=3.6 scikit-learn pandas numpy\n",
    "conda activate fair\n",
    "pip install azureml-sdk[notebooks] azureml-contrib-fairness fairlearn==0.4.6\n",
    "conda install -y ipykernel\n",
    "ipython kernel install --user --name=aml-fair\n",
    "conda deactivate\n",
    "```\n",
    "\n",
    "Then open this notebook and select the **aml-fair** kernel before running the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n",
    "You'll start by training a classification model to predict the likelihood of diabetes. In addition to splitting the data into training a test sets of features and labels, you'll extract *sensitive* features that are used to define subpopulations of the data for which you want to compare fairness. In this case, you'll use the **Age** column to define two categories of patient: those over 50 years old, and those 50 or younger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# load the diabetes dataset\n",
    "print(\"Loading Data...\")\n",
    "data = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "features = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']\n",
    "X, y = data[features].values, data['Diabetic'].values\n",
    "\n",
    "# Get sensitive features\n",
    "A = data[['Age']].astype(int)\n",
    "# Change value to represent age groups\n",
    "A['Age'] = np.where(A.Age > 50, 'Over 50', '50 or younger')\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(X, y, A, test_size=0.20, random_state=0, stratify=y)\n",
    "\n",
    "# Train a classification model\n",
    "print(\"Training model...\")\n",
    "base_model = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the sensitive features with the FairLearn package to compare the model's predictive performance for the different patient categories. To do this, you'll use the FairLearn dashboard:\n",
    "\n",
    "1. Run the cell below.\n",
    "2. When the widget is displayed, use the **Get started** link to start configuring your visualization.\n",
    "3. Select the sensitive features you want to compare (in this case, there's only one: **Age**).\n",
    "4. Select the model performance metric you want to compare (in this case, it's a binary classification model so the options are *Accuracy*, *Balanced accuracy*, *Precision*, and *Recall*)\n",
    "5. View the dashboard visualization, which shows:\n",
    "    - **Disparity in performance** - how the selected performance metric compares for the subpopulations, including *underprediction* (false negatives) and *overprediction* (false positives).\n",
    "    - **Disparity in predictions** - A comparison of the number of positive cases per subpopulation.\n",
    "6. Edit the configuration to compare the predictions based on a different performance metric.\n",
    "\n",
    "> **Note**: We're deliberately not training an optimal model so you can see the disparity in the prediction performance for the two age groups in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.widget import FairlearnDashboard\n",
    "\n",
    "# View this model in Fairlearn's fairness dashboard, and see the disparities which appear:\n",
    "FairlearnDashboard(sensitive_features=A_test, \n",
    "                   sensitive_feature_names=['Age'],\n",
    "                   y_true=y_test,\n",
    "                   y_pred={\"diabetes_model\": base_model.predict(X_test)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whichever performance metric you choose, the model tends to predict positively for patients who are over 50 years old, potentially subjecting this subpopulation to a larger volume of unnecessary diabetes tests.\n",
    "\n",
    "> **Note**: In reality, age is a genuine factor in diabetes, so you would expect more positive cases among older patients; however, the model exhibits some evidence of overpredicting positive cases for the older subpopulation.\n",
    "\n",
    "Let's see what happens if we exclude the **Age** feature when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "features2 = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree']\n",
    "X2, y2 = data[features2].values, data['Diabetic'].values\n",
    "\n",
    "# Get sensitive features\n",
    "A2 = data[['Age']].astype(int)\n",
    "# Change value to represent age groups\n",
    "A2['Age'] = np.where(A2.Age > 50, 'Over 50', '50 or younger')\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train2, X_test2, y_train2, y_test2, A_train2, A_test2 = train_test_split(X2, y2, A2, test_size=0.20, random_state=0, stratify=y2)\n",
    "\n",
    "# Train a classification model\n",
    "print(\"Training model...\")\n",
    "model2 = LogisticRegression(solver='liblinear').fit(X_train2, y_train2)\n",
    "\n",
    "print(\"Model trained.\")\n",
    "\n",
    "# View this model in Fairlearn's fairness dashboard, and see the disparities which appear:\n",
    "FairlearnDashboard(sensitive_features=A_test2, \n",
    "                   sensitive_feature_names=['Age'],\n",
    "                   y_true=y_test2,\n",
    "                   y_pred={\"diabetes_model2\": model2.predict(X_test2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model significantly underpredicts positive cases for older patients, showing that even though **Age** was not a feature used in training, the model still exhibits disparity in how well it predicts for older and younger patients. \n",
    "\n",
    "The overall predictive performance of the model has diminished, so clearly **Age** *is* a predictive feature - we just need to mitigate the tendency for the model to overpredict positive labels for older patients.\n",
    "\n",
    "## Register the model and upload the dashboard to Azure Machine Learning\n",
    "\n",
    "You've trained the model and reviewed the dashboard locally in this notebook; but it might be useful to register the model in your Azure Machine Learning workspace and create an experiement to analyze it for fairness so you can keep track of your mitigation strategy.\n",
    "\n",
    "Let's start by registering the original model (which included **Age** as a feature).\n",
    "\n",
    "> **Note**: If prompted, follow the link and enter the authentication code provided to sign into your Azure subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Model\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load the Azure ML workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to work with', ws.name)\n",
    "\n",
    "# Save the trained model\n",
    "model_file = 'diabetes_model.pkl'\n",
    "joblib.dump(value=base_model, filename=model_file)\n",
    "\n",
    "# Register the model\n",
    "print('Registering model...')\n",
    "registered_model = Model.register(model_path=model_file,\n",
    "                                    model_name='diabetes_classifier',\n",
    "                                    workspace=ws)\n",
    "model_id= registered_model.id\n",
    "\n",
    "\n",
    "print('Model registered.', model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the FairLearn package to create a group of metrics for one or more models, and use an Azure Machine Learning experiment to upload the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics._group_metric_set import _create_group_metric_set\n",
    "from azureml.contrib.fairness import upload_dashboard_dictionary, download_dashboard_by_upload_id\n",
    "\n",
    "#  Create a dictionary of model(s) you want to assess for fairness \n",
    "sf = { 'Age': A_test.Age}\n",
    "ys_pred = { model_id:base_model.predict(X_test) }\n",
    "dash_dict = _create_group_metric_set(y_true=y_test,\n",
    "                                    predictions=ys_pred,\n",
    "                                    sensitive_features=sf,\n",
    "                                    prediction_type='binary_classification')\n",
    "\n",
    "exp = Experiment(ws, \"Diabetes_Fairness\")\n",
    "print(exp)\n",
    "\n",
    "run = exp.start_logging()\n",
    "\n",
    "# Upload the dashboard to Azure Machine Learning\n",
    "try:\n",
    "    dashboard_title = \"Fairness insights of Diabetes Classifier\"\n",
    "    upload_id = upload_dashboard_dictionary(run,\n",
    "                                            dash_dict,\n",
    "                                            dashboard_name=dashboard_title)\n",
    "    print(\"\\nUploaded to id: {0}\\n\".format(upload_id))\n",
    "\n",
    "    # To test the dashboard, you can download it back and ensure it contains the right information\n",
    "    downloaded_dict = download_dashboard_by_upload_id(run, upload_id)\n",
    "    print(downloaded_dict)\n",
    "finally:\n",
    "    run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding code downloaded the metrics generated in the experiement just to confirm it completed successfully. The real benefit of uploading the metrics to an experiement is that you can now view the FairLearn dashboard in Azure Machine Learning studio.\n",
    "\n",
    "Run the cell below to see the output of the experiment, and click the link to see the run in Azure Machine Learning studio. Then view the **Fairness** tab of the experiment run to view the dashboard, which behaves the same way as the widget you viewed previously in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find the fairness dashboard by selecting a model in the **Models** page of Azure Machine Learning studio and reviewing its **Fairness** tab. This enables your organization to maintain a log of fairness analysis for the models you train and register."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigate Unfairness in the Model\n",
    "\n",
    "Now that you've analyzed the model for fairness, you can use any of the *mitigation* techniques supported by the FairLearn package to find a model that achieves the best balance of predictive performance and fairness.\n",
    "\n",
    "In this exercise, we'll use the **GridSearch** feature, which trains multiple models in an attempt to minimize the disparity of predictive performance for the sensitive features in the dataset (in this case, the age groups).\n",
    "\n",
    "> *This may take some time to run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import GridSearch, DemographicParity, ErrorRate\n",
    "\n",
    "print('Finding mitigated models...')\n",
    "\n",
    "# Train multiple models\n",
    "sweep = GridSearch(LogisticRegression(solver='liblinear'),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid_size=71)\n",
    "\n",
    "sweep.fit(X_train, y_train, sensitive_features=A_train.Age)\n",
    "models = sweep._predictors\n",
    "\n",
    "# Examine the models, finding the error and disparities in each based on the sensitive features\n",
    "errors, disparities = [], []\n",
    "for m in models:\n",
    "    classifier = lambda X: m.predict(X)\n",
    "    \n",
    "    error = ErrorRate()\n",
    "    error.load_data(X_train, pd.Series(y_train), sensitive_features=A_train.Age)\n",
    "    disparity = DemographicParity() #use the DemographicParity constraint to define the mitigation strategy\n",
    "    disparity.load_data(X_train, pd.Series(y_train), sensitive_features=A_train.Age)\n",
    "    \n",
    "    errors.append(error.gamma(classifier)[0])\n",
    "    disparities.append(disparity.gamma(classifier).max())\n",
    "    \n",
    "all_results = pd.DataFrame( {\"model\": models, \"error\": errors, \"disparity\": disparities})\n",
    "\n",
    "# Retain only the most dominant models (those with lower errors than others with the same or lower disparity)\n",
    "dominant_models_dict = dict()\n",
    "base_name_format = \"diabetes_model_{0}\"\n",
    "row_id = 0\n",
    "for row in all_results.itertuples():\n",
    "    model_name = base_name_format.format(row_id)\n",
    "    errors_for_lower_or_eq_disparity = all_results[\"error\"][all_results[\"disparity\"]<=row.disparity]\n",
    "    if row.error <= errors_for_lower_or_eq_disparity.min():\n",
    "        dominant_models_dict[model_name] = row.model\n",
    "    row_id = row_id + 1\n",
    "\n",
    "# Create dictionaries of the mitigated models and predictions (plus the original unmitigated one for comparison)\n",
    "predictions_dominant = {\"diabetes_unmitigated\": base_model.predict(X_test)}\n",
    "models_dominant = {\"diabetes_unmitigated\": base_model}\n",
    "for name, model in dominant_models_dict.items():\n",
    "    value = model.predict(X_test)\n",
    "    predictions_dominant[name] = value\n",
    "    models_dominant[name] = model\n",
    "\n",
    "# Display the model names\n",
    "for model_name in models_dominant:\n",
    "    print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the FairLearn dashboard to compare the mitigated models:\n",
    "\n",
    "Run the cell below and use the wizard to display the models. They are shown as a scatter plot that helps you compare the model performance (based on your chosen metric) with the level of disparity in the model's predictions for the sensitive feature groups (in this case, age ranges). You can select an individual point to see a breakdown of the predictive performance and disparity by sensitive feature for that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FairlearnDashboard(sensitive_features=A_test, \n",
    "                   sensitive_feature_names=['Age'],\n",
    "                   y_true=y_test.tolist(),\n",
    "                   y_pred=predictions_dominant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the model comparison, you can choose the right balance of predictive performance and fairness for your particular needs, and deploy the most appropriate model.\n",
    "\n",
    "## Upload the Mitigation Dashboard to Azure Machine Learning\n",
    "\n",
    "As before, you might want to keep track of your mitigation experimentation. To do this, you can:\n",
    "\n",
    "1. Register the models found by the GridSearch process.\n",
    "2. Compute the performance and disparity metrics for the models.\n",
    "3. Upload the metrics in an Azure Machine Learning experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the models\n",
    "os.makedirs('models', exist_ok=True)\n",
    "registered_models = dict()\n",
    "for name, model in models_dominant.items():\n",
    "    model_file = \"models/{0}.pkl\".format(name)\n",
    "    joblib.dump(value=model, filename=model_file)\n",
    "    registered_model = Model.register(model_path=model_file,\n",
    "                                    model_name=name,\n",
    "                                    workspace=ws)\n",
    "    registered_models[name] = registered_model.id\n",
    "\n",
    "# Get the computed metrics calculated previously \n",
    "prediction_metrics = dict()\n",
    "for name, y_pred in predictions_dominant.items():\n",
    "    prediction_metrics[registered_models[name]] = y_pred\n",
    "\n",
    "#  Create a dictionary of model(s) you want to assess for fairness \n",
    "sf = { 'Age': A_test.Age}\n",
    "dash_dict = _create_group_metric_set(y_true=y_test,\n",
    "                                     predictions=prediction_metrics,\n",
    "                                     sensitive_features=sf,\n",
    "                                     prediction_type='binary_classification')\n",
    "\n",
    "exp = Experiment(ws, \"Diabetes_Fairness_Mitigation\")\n",
    "print(exp)\n",
    "\n",
    "run = exp.start_logging()\n",
    "\n",
    "# Upload the dashboard to Azure Machine Learning\n",
    "try:\n",
    "    dashboard_title = \"Fairness Comparison of Diabetes Models\"\n",
    "    upload_id = upload_dashboard_dictionary(run,\n",
    "                                            dash_dict,\n",
    "                                            dashboard_name=dashboard_title)\n",
    "    print(\"\\nUploaded to id: {0}\\n\".format(upload_id))\n",
    "\n",
    "    # To test the dashboard, you can download it back and ensure it contains the right information\n",
    "    downloaded_dict = download_dashboard_by_upload_id(run, upload_id)\n",
    "    print(downloaded_dict)\n",
    "finally:\n",
    "    run.complete()\n",
    "    RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the experiement has finished running, click the link in the widget at the bottom of the output to view the run in Azure Machine Learning studio, and view the FairLearn dashboard on the **fairness** tab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml-fair",
   "language": "python",
   "name": "aml-fair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
